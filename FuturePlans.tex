\section{Планы на будущее}

В ходе проектирования и реализации алгоритма были отмечен ряд проблем.

Корректность алгоритма в некоторой степени завязана на неубывании времени в системе, для чего пришлось реализовывать собственный сервис, который дает необходимые гарантии. Но существует сценарий развития событий, при котором этот сервис будет отдавать одно и то же значение на протяжении нескольких минут: время на репликах рассинхронизировалось и на одной из них убежало вперед на 10 минут относительно остальных, после чего эта реплика вышла из строя. В результате на протяжении следующих 10 минут значение в ячейке обновляться не будет, так как оно больше чем локальное время на всех остальных репликах.

Проблема рассинхронизации времени также всплывает в логике аренды блокировки: в силу внутреннего устройства Cassandra если время на репликах разъедется больше чем на TTL, то ячейка будет тут же удаляться из строки, тем самым нарушив корректность работы алгоритма блокировки. Поэтому TTL приходится указывать достаточно большой (около 15 минут), что вообще говоря не очень хорошо по очевидным причинам.

В стадии разработки сейчас находится решение, в котором логика аренды блокировки лежит не на TTL, а на собственном сервисе, который следит за тем, что аренда продлевается. Однако в ходе реализации этого сервиса стали возникать проблемы, в ходе решения которых становится понятно, что в принципе логику с проверкой активности захватившего блокировку потока лучше решать на стороне хранилища. Более того, сам алгоритм постоянно обращается к базе данных, что заметно повышает нагрузку на нее. Количество запросов к хранилищу можно было бы снизить, если бы оно поддерживало механизм обратной связи: поток сообщает хранилищу, что он хочет захватить блокировку, и через некоторое время хранилище само говорит потоку, что блокировка была успешно взята. К сожалению, Cassandra не имеет подобного механизма.

Таким образом в планах реализация своего распределенного хранилища, заточенного исключительно под распределенные блокировки.